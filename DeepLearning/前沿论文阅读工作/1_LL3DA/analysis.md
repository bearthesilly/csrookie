# LL3DA

## 前言 

**摘要：** 最近在大型多模态模型（LMM）方面的进展为各种人工智能应用在人机交互领域的应用提供了可能性。然而，开发能够理解、推理和规划复杂和多样化的3D环境的LMM仍然是一个挑战，特别是考虑到对3D场景中排列不变的点云3D表示的理解需求。现有的研究工作通过多视图图像寻求帮助，并将2D特征投影到3D空间作为3D场景的表示。然而，这导致了巨大的计算开销和性能下降。在本文中，我们提出了LL3DA，这是一个大型语言3D助手，**它采用点云作为直接输入，并响应文本指令和视觉提示。这有助于LMM更好地理解人类交互，并进一步帮助消除复杂3D场景中的歧义。**实验表明，LL3DA取得了显著的结果，并在3D密集字幕（3D Dense Captioning）和3D问答（3D Question Answering）上超越了各种3D视觉-语言模型。

以往处理点云模型都是通过对三维模型进行二维"摄影"然后对着图片进行分析. 但是弊端很多, 计算量大, 而且采取的角度也有讲究, 更难以处理的是, 不同的角度拍摄的图片所处理后得到的信息可能随着处理角度的变化而变化. 

**引言：**大型语言模型（LLM）家族的最近增长为以一种通用方式解决各种机器学习任务打开了巨大的机会。在这场LLM盛会中，研究人员也在寻求各种视觉语言任务的通用LLM解决方案。其中，基于LLM的3D场景理解是一个有价值的主题，将有利于自动驾驶和具身AI代理的发展。然而，鉴于3D环境的多样性和复杂性以及对稀疏3D点的理解需求，这同样是一个挑战。以前的工作在解决各种3D视觉和语言任务方面取得了初步的成功。研究的主流是构建3D specialists，旨在解决一个特定的下游任务，包括3D问答（3DQA）、3D视觉定位（3D-VG）和3D密集字幕（***3D-Dense Captioning***）。同时，其他工作研究了不同3D视觉语言任务之间的相互促进，通过在不同任务上同时训练它们的共享架构。最近，研究人员还引入了LLM用于一般性的3D理解，其中**Point-Bind和Point-LLMs**主要关注3D对象的理解。与此同时，**3DLLM**提出了一个由LLM驱动的解决方案，该方案聚合了多视图特征用于3D特征，**展示了机器能够理解各种3D对象和场景**，并遵循人类产生的文本指令的强大能力。尽管这些方法在理解3D世界和自然语言方面取得了显著的成功，但存在某些局限性。在有限的监督下，3D specialists很难扩展以获得更好的性能，而联合预训练仍然需要为特定任务单独的头部。**提取多视图特征导致巨大的计算开销，并忽略了基本的几何属性**。此外，纯文本经常在杂乱和复杂的3D环境中导致歧义。为了解决上述问题，我们提出了LL3DA，一个大型语言3D助手，能够响应人类的文本和视觉互动，并在复杂的3D环境中理解、推理和规划（见图1）。我们采用了一个多模态变换器(**multi-model**)，通过注意力机制将文本指令、视觉提示和3D场景的信息聚合到固定长度的***learnable querying tokens***中。**querying tokens**被投影并用作***文本指令的前缀***，作为***pre-trained and frozen LLM***的输入。这种设计不仅有助于解决排列不变的3D场景嵌入与LLM嵌入空间之间的矛盾，而且还提取了交互感知的3D场景嵌入，以实现有效的指令遵循。我们进行了广泛的实验，探索了LLM在复杂和多样化的3D环境中理解、推理和规划的能力。我们的模型在两个广泛使用的3D密集字幕和3D问答数据集上取得了最先进的结果。此外，通过引入额外的视觉互动，我们的方法可以进一步消除模糊文本指令中的歧义。总结来说，我们的主要贡献在于：

- 我们提出了一个基于LLM的解决方案，用于在复杂的3D环境中理解、推理和规划。
- 我们的模型接受文本指令和视觉互动作为输入，并提取交互感知特征以有效遵循指令。
- 广泛的实验表明，我们的方法超越了各种现有的最先进的3D视觉语言模型。

3D Dense Captioning 是一种计算机视觉和自然语言处理相结合的任务，它要求模型能够接收一个3D场景（例如，由3D点云或网格表示的室内环境）作为输入，并生成描述这个场景的自然语言句子。这种描述不仅需要捕捉场景的视觉内容，还要能够定位和识别场景中的各个物体或元素。 在3D Dense Captioning中，“Dense”一词通常意味着模型生成的描述是密集的，即它详细地描述了场景中的多个对象和它们之间的关系. 例如，对于一个包含沙发、咖啡桌和植物的室内环境，3D Dense Captioning模型可能会生成如下描述：“房间里有一张棕色的沙发，位于两个灰色的脚踏板之间。沙发下方是一块地毯，上面放着几个抱枕。咖啡桌放在沙发前面，桌上有一个花瓶和一些杂志。左边的角落里有一盆高大的植物。”

在机器学习和特别是自然语言处理领域中，"**Frozen LLM**"（冻结的大型语言模型）指的是在训练过程中不更新其权重的大型语言模型. 

## Pipeline 

LL3DA（Large Language 3D Assistant）的网络框架主要由以下几个关键组件构成：

1. **Scene Encoder (E3D)**: 这是一个采用掩码变换器编码器（预训练于ScanNet检测任务）的场景编码器，它将3D点云PC作为输入，并输出3D场景嵌入。
2. **Visual Prompt Encoder**: 该组件处理两种常见的视觉互动——用户点击和3D框注释。用户点击通过3D傅里叶位置嵌入函数进行编码，而框注释则通过预训练的3D对象检测器提取的ROI特征表示。
3. **Multi-Modal Transformer (MMT)**: 这是一个多模态变换器，它的作用是解决排列不变的3D场景嵌入与位置敏感的因果LLM之间的矛盾，桥接冻结的单模态专家之间的差距，并提取交互感知的特征。MMT通过自注意力机制将视觉信息聚合到固定数量的可学习查询令牌中。
4. **LLM (Large Language Model)**: 使用解码器-仅生成预训练变换器作为大型语言模型的主干，参数和嵌入层在训练期间保持冻结状态。
5. **Q-Former**: 这是一个用于将排列不变的3D嵌入转换为固定长度交互感知场景嵌入的架构，作为LLM输入的前缀。
6. **Prompt Encoder**: 该编码器将用户点击和框坐标与位置嵌入和ROI特征一起编码。
7. **Interactor3D**: 这是一个详细设计，它聚合视觉提示、文本指令和3D场景嵌入到固定长度的查询令牌中。
8. **训练目标和指令调整**: 通过适当的训练数据和指令，指导模型生成期望的输出，并通过优化可训练参数来最大化目标响应序列的可能性。

LL3DA的网络框架通过这些组件的相互作用，实现了对复杂3D环境中的文本指令和视觉提示的理解、推理和规划。该框架的设计允许模型直接从3D场景中提取特征，并且能够处理视觉提示和文本指令，以多样化人类与复杂3D环境的可能互动。

以上分析来自kimi

在LL3DA框架中，多模态变换器（MMT）具体是如何工作的，它如何帮助模型理解复杂的3D场景？

在LL3DA框架中，多模态变换器（Multi-Modal Transformer，简称MMT）起着至关重要的作用，它帮助模型理解和处理来自复杂3D场景的视觉和文本指令。MMT的具体工作流程如下：

1. **特征整合**：MMT通过自注意力机制整合来自不同源的特征，包括文本指令、视觉提示（如用户点击或3D框注释）和3D场景的嵌入。
2. **查询令牌生成**：MMT使用一个称为Q-Former的组件将排列不变的3D场景嵌入转换为固定长度的查询令牌。这些查询令牌是通过注意力机制生成的，能够捕捉场景中的重要信息。
3. **交互感知特征提取**：MMT通过交叉注意力机制，允许查询令牌与3D场景嵌入进行交互，从而提取交互感知的特征。这有助于模型更好地理解场景中不同对象之间的关系。
4. **冻结的单模态专家桥接**：MMT桥接了冻结的单模态专家（如3D场景编码器和视觉提示编码器），使得这些专家生成的特征能够与文本指令相结合。
5. **位置敏感的LLM适应**：MMT解决了排列不变的3D嵌入与位置敏感的LLM之间的矛盾，通过生成固定长度的查询令牌，使得LLM能够以一种有序和连贯的方式处理3D场景信息。
6. **输出投影**：MMT的输出是一组查询令牌，这些令牌被投影到LLM的嵌入空间中，作为LLM输入的一部分。
7. **冻结的LLM**：在LL3DA中，大型语言模型（LLM）的参数和嵌入层在训练期间保持冻结，以节省内存成本。MMT的输出作为LLM的输入，使得LLM能够生成自然语言响应。

通过这种方式，MMT使得LL3DA能够处理复杂的3D场景中的多种交互方式，包括文本指令和视觉提示，从而提高了模型对3D环境的理解能力。此外，MMT的设计允许模型在不同的3D视觉语言任务中进行有效的特征提取和指令执行，这对于在多样化的3D环境中进行机器学习和自主决策至关重要。

